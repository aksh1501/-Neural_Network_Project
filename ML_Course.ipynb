{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK6Dr74uWT24+745kTGZ7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksh1501/-Neural_Network_Project/blob/main/ML_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u6a5eLi52Wz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel trick is a technique used in machine learning, particularly in Support Vector Machines (SVMs), to implicitly map input data into higher-dimensional feature spaces without explicitly computing the transformation. This technique allows SVMs to efficiently handle non-linear decision boundaries by transforming the data into a space where a linear decision boundary can be applied.\n",
        "\n",
        "In traditional SVMs, the decision boundary is a hyperplane that separates data points into different classes. However, in many real-world scenarios, the data may not be linearly separable in the original feature space. The kernel trick provides a way to deal with such situations by mapping the input data into a higher-dimensional space where it may become linearly separable.\n",
        "\n",
        "Mathematically, the kernel trick involves replacing the dot product of feature vectors in the input space with a kernel function. The kernel function calculates the similarity between pairs of data points in the input space. The most commonly used kernel functions include:\n",
        "\n",
        "1. **Linear Kernel**: \\( K(x_i, x_j) = x_i^T x_j \\)\n",
        "2. **Polynomial Kernel**: \\( K(x_i, x_j) = (x_i^T x_j + c)^d \\)\n",
        "3. **Gaussian (RBF) Kernel**: \\( K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\)\n",
        "4. **Sigmoid Kernel**: \\( K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c) \\)\n",
        "\n",
        "These kernel functions allow SVMs to implicitly compute the dot products in higher-dimensional spaces without explicitly transforming the input data. This is computationally more efficient, especially when dealing with high-dimensional data or when the explicit transformation into a higher-dimensional space is infeasible due to computational constraints.\n",
        "\n",
        "By using the kernel trick, SVMs can learn complex decision boundaries that may not be possible to represent in the original feature space. This makes SVMs with kernel functions powerful tools for handling non-linear classification tasks and has made them widely used in various machine learning applications."
      ],
      "metadata": {
        "id": "jHaZci2-56pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "etFYBbkFygou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table that is often used to evaluate the performance of a classification model. It compares the actual labels of a dataset with the labels predicted by the model. The matrix consists of four different combinations of predicted and actual classes:\n",
        "\n",
        "1. True Positives (TP): The cases in which the model correctly predicted the positive class.\n",
        "2. True Negatives (TN): The cases in which the model correctly predicted the negative class.\n",
        "3. False Positives (FP): The cases in which the model incorrectly predicted the positive class when the actual class was negative. (Also known as Type I error)\n",
        "4. False Negatives (FN): The cases in which the model incorrectly predicted the negative class when the actual class was positive. (Also known as Type II error)\n",
        "\n",
        "The confusion matrix is typically laid out as follows:\n",
        "\n",
        "```\n",
        "              Predicted Negative   Predicted Positive\n",
        "Actual Negative        TN                   FP\n",
        "Actual Positive        FN                   TP\n",
        "```\n",
        "\n",
        "Here's a brief explanation of each cell in the confusion matrix:\n",
        "\n",
        "- True Negatives (TN): The model correctly predicted negative instances as negative.\n",
        "- False Positives (FP): The model incorrectly predicted negative instances as positive.\n",
        "- False Negatives (FN): The model incorrectly predicted positive instances as negative.\n",
        "- True Positives (TP): The model correctly predicted positive instances as positive.\n",
        "\n",
        "The confusion matrix provides valuable insights into the performance of a classification model, such as accuracy, precision, recall (sensitivity), specificity, and F1 score. These metrics can help assess the model's performance across different classes and identify areas for improvement."
      ],
      "metadata": {
        "id": "D0Tg4G6Oygy4"
      }
    }
  ]
}